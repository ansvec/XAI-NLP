{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import libraries"
      ],
      "metadata": {
        "id": "vGJI900vES_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lime\n",
        "#!pip install elis5\n",
        "#!pip install shap"
      ],
      "metadata": {
        "id": "z30zqsRbPYQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VAZaU51FGgQ",
        "outputId": "bdfbf836-9df3-422a-ae8b-f07cb32d45a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import LSTM, InputLayer, Dense, Embedding, Dropout,SpatialDropout1D, Bidirectional, TimeDistributed, Activation, Masking, Lambda\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from keras.models import load_model\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.models import Model\n",
        "from keras.layers import Input"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "BUMPdi4EEYuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")\n",
        "data.tail(10)"
      ],
      "metadata": {
        "id": "q00o3EarGZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceGetter:\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        self.sentences = self.group_sentences(data)\n",
        "\n",
        "    def group_sentences(self, data):\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        grouped = data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        return [s for s in grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            sentence = self.sentences[self.n_sent - 1]\n",
        "            self.n_sent += 1\n",
        "            return sentence\n",
        "        except IndexError:\n",
        "            return None"
      ],
      "metadata": {
        "id": "F2Rsrgv3G5qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences"
      ],
      "metadata": {
        "id": "lm2lNWuvHC5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = [[s[2] for s in sent] for sent in sentences]\n",
        "pos = [[s[1] for s in sent] for sent in sentences]\n",
        "sentences = [\" \".join(s[0] for s in sent) for sent in sentences]"
      ],
      "metadata": {
        "id": "QpYc9PRpH-D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "\n",
        "def print_sentence_with_pos_and_ner(sentences, pos, ner, index):\n",
        "    # Print the specified sentence with its POS and NER tags\n",
        "    print(\"Sentence: \", sentences[index])\n",
        "    print(\"POS: \", pos[index])\n",
        "    print(\"NER: \", ner[index])\n",
        "\n",
        "print_sentence_with_pos_and_ner(sentences, pos, ner, index)"
      ],
      "metadata": {
        "id": "LsvuY9lcHPb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data analysis and preprocessing\n"
      ],
      "metadata": {
        "id": "53COQJ-BEd-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the length of each sentence\n",
        "sent_lengths = [len(sent.split()) for sent in sentences]\n",
        "\n",
        "# Find the index of the shortest and longest sentences\n",
        "shortest_idx = sent_lengths.index(min(sent_lengths))\n",
        "longest_idx = sent_lengths.index(max(sent_lengths))\n",
        "\n",
        "# Print the shortest sentence and its POS and NER tags\n",
        "print(\"Najkratšia veta v dátovej množine:\")\n",
        "print_sentence_with_pos_and_ner(sentences, pos, ner, shortest_idx)\n",
        "\n",
        "# Print the longest sentence and its POS and NER tags\n",
        "print(\"Najdlhšia veta v dátovej množine:\")\n",
        "print_sentence_with_pos_and_ner(sentences, pos, ner, longest_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUAwD-bMOv5H",
        "outputId": "93f071f8-fb9e-4080-9b56-472b454b50fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Najkratšia veta v dátovej množine:\n",
            "Sentence:  ...\n",
            "POS:  [':']\n",
            "NER:  ['O']\n",
            "Najdlhšia veta v dátovej množine:\n",
            "Sentence:  Fisheries in 2006 - 7 landed 1,26,976 metric tons , of which 82 % ( 1,04,586 tons ) was krill ( Euphausia superba ) and 9.5 % ( 12,027 tons ) Patagonian toothfish ( Dissostichus eleginoides - also known as Chilean sea bass ) , compared to 1,27,910 tons in 2005 - 6 of which 83 % ( 1,06,591 tons ) was krill and 9.7 % ( 12,396 tons ) Patagonian toothfish ( estimated fishing from the area covered by the Convention of the Conservation of Antarctic Marine Living Resources ( CCAMLR ) , which extends slightly beyond the Southern Ocean area ) .\n",
            "POS:  ['NNS', 'IN', 'CD', ':', 'CD', 'VBD', 'CD', 'JJ', 'NNS', ',', 'IN', 'WDT', 'CD', 'NN', 'LRB', 'CD', 'NNS', 'RRB', 'VBD', 'NN', 'LRB', 'NNP', 'NNP', 'RRB', 'CC', 'CD', 'NN', 'LRB', 'CD', 'NNS', 'RRB', 'JJ', 'NN', 'LRB', 'NNP', 'NNP', ':', 'RB', 'VBN', 'IN', 'JJ', 'NN', 'NN', 'RRB', ',', 'VBN', 'TO', 'CD', 'NNS', 'IN', 'CD', ':', 'CD', 'IN', 'WDT', 'CD', 'NN', 'LRB', 'CD', 'NNS', 'RRB', 'VBD', 'NN', 'CC', 'CD', 'NN', 'LRB', 'CD', 'NNS', 'RRB', 'JJ', 'NN', 'LRB', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'LRB', 'NNP', 'RRB', ',', 'WDT', 'VBZ', 'RB', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'RRB', '.']\n",
            "NER:  ['O', 'O', 'B-tim', 'I-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nat', 'I-nat', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-nat', 'I-nat', 'B-per', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'B-org', 'I-org', 'I-org', 'I-org', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of words in the shortest sentence\n",
        "shortest_words = sentences[shortest_idx].split()\n",
        "num_shortest_words = len(shortest_words)\n",
        "\n",
        "# Count the number of words in the longest sentence\n",
        "longest_words = sentences[longest_idx].split()\n",
        "num_longest_words = len(longest_words)\n",
        "\n",
        "# Print the results\n",
        "print(\"Počet slov v najkratšej vete:\", num_shortest_words)\n",
        "print(\"Počet slov v najdlhšej vete:\", num_longest_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrZqQRgWPHUj",
        "outputId": "697d963e-33c0-4a20-d1c9-522872d0c333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Počet slov v najkratšej vete: 1\n",
            "Počet slov v najdlhšej vete: 104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#identifying 4000 most common words\n",
        "word_counter = Counter(data[\"Word\"].values)\n",
        "vocabulary = set([word[0] for word in word_counter.most_common(4000)])\n",
        "words = list(set(data[\"Word\"].values))\n",
        "word_count= len(words)"
      ],
      "metadata": {
        "id": "hm-O_IeOa_rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 50\n",
        "\n",
        "# word2idx mapping of vocabulary\n",
        "word2idx = {\"PAD\": 0, \"UNK\": 1}\n",
        "word2idx.update({w: i+2 for i, w in enumerate(vocabulary)})"
      ],
      "metadata": {
        "id": "n3uTncZNeiey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tags = list(set(data[\"Tag\"].values))"
      ],
      "metadata": {
        "id": "1AIZTzfbfo-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = list(set(data[\"POS\"].values))"
      ],
      "metadata": {
        "id": "PBBg0X2qf6f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tag2idx = {ner_tag: i + 1 for i, ner_tag in enumerate(ner_tags)}\n",
        "ner_tag2idx['<PAD>'] = 0"
      ],
      "metadata": {
        "id": "z3xP2bjcc2Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag2idx = {pos_tag: i + 1 for i, pos_tag in enumerate(pos_tags)}\n",
        "pos_tag2idx['<PAD>'] = 0"
      ],
      "metadata": {
        "id": "ogsNvzU0hNEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to numerical values\n",
        "X = [[word2idx.get(word, word2idx[\"UNK\"]) for word in sentence.split()] for sentence in sentences]\n",
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])"
      ],
      "metadata": {
        "id": "DVnyQYtojbk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_seq = pad_sequences(X,maxlen=max_len, padding=\"post\", value=word2idx[\"PAD\"])"
      ],
      "metadata": {
        "id": "mKxLFKpvrL0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = list(map(lambda z: [ner_tag2idx[z_i] for z_i in z], ner))\n",
        "y_seq = pad_sequences(y, maxlen=max_len, padding=\"post\", value=ner_tag2idx[\"O\"])"
      ],
      "metadata": {
        "id": "2ldM1aRbssD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, stratify=y_seq[:,1])"
      ],
      "metadata": {
        "id": "kFCSsVkOuIWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmGXoGuJuplK",
        "outputId": "63363f66-d1b6-4202-a057-451130726137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (38367, 50)\n",
            "X_test shape: (9592, 50)\n",
            "y_train shape: (38367, 50)\n",
            "y_test shape: (9592, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_ner_train = to_categorical(y_train, num_classes=len(ner_tag2idx))\n",
        "y_ner_test = to_categorical(y_test, num_classes=len(ner_tag2idx))"
      ],
      "metadata": {
        "id": "5nj_HI-S73Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "x2MK6XU5Eqcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#POS Bi-LSTM model2 with regularization:\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(max_len, )))\n",
        "model.add(Embedding(input_dim=word_count+1, output_dim=50, input_length=max_len, dtype='float32'))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True,kernel_regularizer=l2(0.0000001), recurrent_regularizer=l1(0.0000001))))\n",
        "model.add(LSTM(128, return_sequences=True,kernel_regularizer=l2(0.0000001)))\n",
        "model.add(TimeDistributed(Dense(len(ner_tags)+1, activation='softmax')))\n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'], loss_weights=[0.5, 0.5])"
      ],
      "metadata": {
        "id": "fGVjSCl61UBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "OqPLVGg7vesT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    to_file='model.png',\n",
        "    show_shapes=True,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir='TB',\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        "    show_layer_activations=True,\n",
        "    show_trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "2BxFOMXyvj8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = 'ner.hdf5'\n",
        "checkpoint = ModelCheckpoint(saved_model, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "history = model.fit(X_train, y_ner_train, validation_data=(X_test,y_ner_test), epochs=epochs, batch_size=batch_size,callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "rFLLpDDWvoYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainingProcess(history):\n",
        "    plt.style.use('ggplot')\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.style.use('ggplot')\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d6Vth8LPAjol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainingProcess(history)"
      ],
      "metadata": {
        "id": "pyyEVuX6AmiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation"
      ],
      "metadata": {
        "id": "ofepv29eFFXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred_ner_test = model.predict(X_test)\n",
        "y_pred_ner_test = np.argmax(y_pred_ner_test, axis=-1)"
      ],
      "metadata": {
        "id": "BDrVeeFBA8c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ner_test.shape"
      ],
      "metadata": {
        "id": "MxyxiPB2BwWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ner_test = np.argmax(y_ner_test, axis=-1)"
      ],
      "metadata": {
        "id": "umQFrEKoCI3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ner_test.shape"
      ],
      "metadata": {
        "id": "bHmbVEcZCTWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Results of Test B Set\n",
        "print('Evaluation for test set')\n",
        "print('Accuracy score:', accuracy_score(y_ner_test.flatten(), y_pred_ner_test.flatten()))\n",
        "print('Precision score:', precision_score(y_ner_test.flatten(), y_pred_ner_test.flatten(), average='macro',labels=np.unique(y_pred_ner_test)))\n",
        "print('Recall score:', recall_score(y_ner_test.flatten(), y_pred_ner_test.flatten(), average='weighted'))\n",
        "print('F1 score:', f1_score(y_ner_test.flatten(), y_pred_ner_test.flatten(), average='macro', labels=np.unique(y_pred_ner_test)))\n",
        "#print('ROC AUC score:', roc_auc_score(y_ner_testb.flatten(), y_pred_ner_testb.flatten(), multi_class='ovr'))\n",
        "#print('Confusion matrix:\\n', confusion_matrix(y_ner_testb.flatten(), y_pred_ner_testb.flatten()))\n",
        "print('Classification report:\\n', classification_report(y_ner_test.flatten(), y_pred_ner_test.flatten(), target_names =ner_tag2idx))"
      ],
      "metadata": {
        "id": "IqQjbIy4BE7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print('Classification report:\\n', classification_report(y_ner_test.flatten(), y_pred_ner_test.flatten(), target_names = list(ner_tags)))"
      ],
      "metadata": {
        "id": "Vh_BtcoMA1Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "def monkeypath_itemfreq(sampler_indices):\n",
        "   return zip(*np.unique(sampler_indices, return_counts=True))\n",
        "\n",
        "scipy.stats.itemfreq=monkeypath_itemfreq"
      ],
      "metadata": {
        "id": "N75fELWo_7cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from eli5.lime.samplers import MaskingTextSampler   "
      ],
      "metadata": {
        "id": "kPaZlsWYHl-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        " \n",
        "explainer = LimeTextExplainer(class_names = ner_tags)\n",
        "\n",
        "exp = explainer.explain_instance(sentences[index], predict_func)\n",
        "\n",
        "print(exp.as_list())"
      ],
      "metadata": {
        "id": "b7lT4T_AM5_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp.show_in_notebook(text=True)"
      ],
      "metadata": {
        "id": "XDl0RJgbNmEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_set_feature = explainer.explain_instance(sentences[index], predict_func, num_features=9)\n",
        "\n",
        "print(exp_set_feature.as_list())\n",
        "exp_set_feature.show_in_notebook(text=True)"
      ],
      "metadata": {
        "id": "wFuyrGGpSQ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 46781\n",
        "ner_idx = ner[index]\n",
        "sentence_text = sentences[index]\n",
        "print('Veta bez priradenia pomenovaných entít:')\n",
        "print(sentence_text)\n",
        "print('Veta s NER:')\n",
        "print(\" \".join([f\"{t} ({l})\" for t, l in zip(sentence_text.split(), ner_idx)]))"
      ],
      "metadata": {
        "id": "KmegozinM9D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERExplainerGenerator(object):\n",
        "    \n",
        "    def __init__(self, model, word2idx, tag2idx, max_len):\n",
        "        self.model = model\n",
        "        self.word2idx = word2idx\n",
        "        self.ner_tag2idx = ner_tag2idx\n",
        "        self.idx2tag = {v: k for k,v in tag2idx.items()}\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def _preprocess(self, texts):\n",
        "        X = [[self.word2idx.get(w, self.word2idx[\"UNK\"]) for w in t.split()]\n",
        "             for t in texts]\n",
        "        X = pad_sequences(maxlen=self.max_len, sequences=X,\n",
        "                          padding=\"post\", value=self.word2idx[\"PAD\"])\n",
        "        return X\n",
        "    \n",
        "    def get_predict_function(self, word_index):\n",
        "        def predict_func(texts):\n",
        "            X = self._preprocess(texts)\n",
        "            p = self.model.predict(X)\n",
        "            return p[:,word_index,:]\n",
        "        return predict_func"
      ],
      "metadata": {
        "id": "1v6BAFEsMQIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, w in enumerate(sentence_text.split()):\n",
        "    print(f\"{i}: {w}\")"
      ],
      "metadata": {
        "id": "FSetMT1WFqKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer_generator = NERExplainerGenerator(model, word2idx, ner_tag2idx, max_len)"
      ],
      "metadata": {
        "id": "BRvyqlunGK2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = 4\n",
        "predict_func = explainer_generator.get_predict_function(word_index=word_index)"
      ],
      "metadata": {
        "id": "Aynrz5L-GTLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('ner (1).hdf5')\n",
        "\n",
        "c = make_pipeline(model)\n",
        "\n",
        "text = \"This is an example sentence.\"\n",
        "\n",
        "words = text.split()\n",
        "explainer = LimeTextExplainer(class_names=ner_tags)\n",
        "\n",
        "\n",
        "# Function to generate explanations for a word in a sentence\n",
        "def explain_word(word_index, words, explainer, pipeline):\n",
        "    modified_sentence = ' '.join(words[:word_index] + ['__TARGET__'] + words[word_index+1:])\n",
        "    exp = explainer.explain_instance(sentence[index], predict_fun, num_features=word_count, labels=ner_tag2idx)\n",
        "    return exp.as_list(label=1)\n",
        "\n",
        "# Generate explanations for each word\n",
        "explanations = [explain_word(i, words, explainer, c) for i in range(len(words))]\n",
        "\n",
        "# Aggregate explanations\n",
        "aggregate_explanations = {}\n",
        "for explanation in explanations:\n",
        "    for word, importance in explanation:\n",
        "        if word not in aggregate_explanations:\n",
        "            aggregate_explanations[word] = 0\n",
        "        aggregate_explanations[word] += importance\n",
        "\n",
        "# Normalize the aggregated importance scores\n",
        "normalized_importance = {word: importance/len(words) for word, importance in aggregate_explanations.items()}\n",
        "\n",
        "print(normalized_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "lB1lweUEoDG3",
        "outputId": "b7db8bad-2ad8-4856-d461-cc54475fbdf8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f264fd98b291>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lime'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}